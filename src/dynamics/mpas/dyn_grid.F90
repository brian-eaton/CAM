#define MPAS_DEBUG_WRITE(print_task, x) if (iam == (print_task)) write(iulog,*) 'MPAS_DEBUG '//subname//' ', (x)

module dyn_grid

!-------------------------------------------------------------------------------
!
! Define MPAS computational grids on the dynamics decomposition.
!
!
! Module responsibilities:
!
! . Provide the physics/dynamics coupler (in module phys_grid) with data for the
!   physics grid on the dynamics decomposition.
!
! . Create CAM grid objects that are used by the I/O functionality to read
!   data from an unstructured grid format to the dynamics data structures, and
!   to write from the dynamics data structures to unstructured grid format.  The
!   global column ordering for the unstructured grid is determined by the dycore.
!
!-------------------------------------------------------------------------------

use shr_kind_mod,      only: r8 => shr_kind_r8
use spmd_utils,        only: iam, masterproc, mpicom, npes

use pmgrid,            only: plev, plevp
use physconst,         only: pi, rearth

use cam_logfile,       only: iulog
use cam_abortutils,    only: endrun

use pio,               only: file_desc_t, var_desc_t, &
                             pio_inq_dimid, pio_inq_dimlen, pio_inq_varid, &
                             pio_double, pio_def_dim, pio_def_var, &
                             pio_put_var, pio_get_var, &
                             pio_seterrorhandling, PIO_BCAST_ERROR, PIO_NOERR

use cam_mpas_subdriver, only: domain_ptr, cam_mpas_init_phase3, cam_mpas_get_global_dims, &
                              cam_mpas_get_global_coords, cam_mpas_get_global_blocks,     &
                              cam_mpas_read_static, cam_mpas_compute_unit_vectors

use mpas_pool_routines, only: mpas_pool_get_subpool, mpas_pool_get_dimension, mpas_pool_get_array
use mpas_derived_types, only: mpas_pool_type


implicit none
private
save

integer, parameter :: dyn_decomp  = 101 ! cell center grid
integer, parameter :: edge_decomp = 102 ! edge node grid
integer, parameter :: ptimelevels = 2

public :: &
   dyn_decomp, &
   ptimelevels, &
   maxNCells,   &
   dyn_grid_init, &
   get_block_bounds_d, &
   get_block_gcol_d, &
   get_block_gcol_cnt_d, &
   get_block_lvl_cnt_d, &
   get_block_levels_d, &
   get_block_owner_d, &
   get_gcol_block_d, &
   get_gcol_block_cnt_d, &
   get_horiz_grid_dim_d, &
   get_horiz_grid_d, &
   get_dyn_grid_parm, &
   get_dyn_grid_parm_real1d, &
   dyn_grid_get_elem_coords, &
   dyn_grid_get_colndx, &
   physgrid_copy_attributes_d

! vertical reference heights (m)
real(r8) :: zw(plevp), zw_mid(plev)

integer ::      &
   maxNCells,   &    ! maximum number of cells for any task
   maxEdges,    &    ! maximum number of edges per cell
   nVertLevels       ! number of vertical layers (yes, layers and not layer interfaces...)

integer, pointer :: &
   nCellsSolve,     &
   nEdgesSolve,     &
   nVerticesSolve,  &
   nVertLevelsSolve

real(r8), parameter :: rad2deg=180.0_r8/pi ! convert radians to degrees

! global grid data

integer ::      &
   nCells_g,    &    ! global number of cells/columns
   nEdges_g,    &    ! global number of edges
   nVertices_g       ! global number of vertices

integer, public, allocatable :: col_indices_in_block(:,:)  !  global column indices (used in dp_coupling)
integer,         allocatable :: num_col_per_block(:)
integer,         allocatable :: global_blockid(:)
integer, public, allocatable :: local_col_index(:)  !  local to block

real(r8), dimension(:), pointer :: lonCell_g        ! global cell longitudes
real(r8), dimension(:), pointer :: latCell_g        ! global cell latitudes
real(r8), dimension(:), pointer :: areaCell_g       ! global cell areas

!=========================================================================================
contains
!=========================================================================================

subroutine dyn_grid_init()

   ! Initialize grids on the dynamics decomposition and create associated
   ! grid objects for use by I/O utilities.  The current physics/dynamics
   ! coupling code requires constructing global fields for the grid used
   ! by the physics parameterizations.

   use ref_pres,            only: std_atm_pres, ref_pres_init
   use time_manager,        only: get_step_size

   use cam_initfiles,       only: initial_file_get_id

   use cam_history_support, only: add_vert_coord

   type(file_desc_t), pointer :: fh_ini

   integer  :: k
   integer  :: num_pr_lev       ! number of top levels using pure pressure representation
   real(r8) :: pref_edge(plevp) ! reference pressure at layer edges (Pa)
   real(r8) :: pref_mid(plev)   ! reference pressure at layer midpoints (Pa)

   character(len=*), parameter :: subname = 'dyn_grid::dyn_grid_init'
   !----------------------------------------------------------------------------

   MPAS_DEBUG_WRITE(0, 'begin '//subname)


   ! Get filehandle for initial file
   fh_ini => initial_file_get_id()

   call cam_mpas_init_phase3(fh_ini, endrun)

   ! Read or compute all time-invariant fields for the MPAS-A dycore
   ! Time-invariant fields are stored in the MPAS mesh pool.  This call
   ! also sets the module data zw and zw_mid.
   call setup_time_invariant(fh_ini)

   ! Compute reference pressures from reference heights.
   call std_atm_pres(zw, pref_edge)
   pref_mid = (pref_edge(1:plev) + pref_edge(2:plevp)) * 0.5_r8

   num_pr_lev = 0
   call ref_pres_init(pref_edge, pref_mid, num_pr_lev)

   ! Vertical coordinates for output streams
   call add_vert_coord('lev', plev,                                         &
         'zeta level at vertical midpoints', 'm', zw_mid)
   call add_vert_coord('ilev', plevp,                                       &
         'zeta level at vertical interfaces', 'm', zw)

   if (masterproc) then
      write(iulog,'(a)')' Reference Layer Locations: '
      write(iulog,'(a)')' index      height (m)              pressure (hPa) '
      do k= 1, plev
         write(iulog,9830) k, zw(k), pref_edge(k)/100._r8
         write(iulog,9840)    zw_mid(k), pref_mid(k)/100._r8
      end do
      write(iulog,9830) plevp, zw(plevp), pref_edge(plevp)/100._r8
   end if

   ! Query global grid dimensions from MPAS
   call cam_mpas_get_global_dims(nCells_g, nEdges_g, nVertices_g, maxEdges, nVertLevels, maxNCells)

   ! Temporary global arrays needed by phys_grid_init
   allocate(lonCell_g(nCells_g))
   allocate(latCell_g(nCells_g))
   allocate(areaCell_g(nCells_g))
   call cam_mpas_get_global_coords(latCell_g, lonCell_g, areaCell_g)
   
   allocate(num_col_per_block(npes))
   allocate(col_indices_in_block(maxNCells,npes))
   allocate(global_blockid(nCells_g))
   allocate(local_col_index(nCells_g))
   call cam_mpas_get_global_blocks(num_col_per_block, col_indices_in_block, global_blockID, local_col_index)
   
   ! Define the dynamics and physics grids on the dynamics decompostion.
   ! Physics grid on the physics decomposition is defined in phys_grid_init.
   call define_cam_grids()
   
9830 format(1x, i3, f15.4, 9x, f15.4)
9840 format(1x, 3x, 12x, f15.4, 9x, f15.4)

end subroutine dyn_grid_init


!-----------------------------------------------------------------------
!  routine get_block_bounds_d
!
!> \brief Return first and last indices used in global block ordering
!> \details
!>  Return first and last indices used in global block ordering.
!>
!>  Are these the blocks owned by the calling MPI task, or all blocks
!>  across all MPI tasks?
!>
!>  Is this a 1-based ordering? Why would the first block ever not be 1?
!
!-----------------------------------------------------------------------
subroutine get_block_bounds_d( block_first, block_last )

   integer, intent(out) :: block_first  ! first global index used for blocks
   integer, intent(out) :: block_last   ! last global index used for blocks

   character(len=*), parameter :: subname = 'dyn_grid::get_block_bounds_d'


!   MPAS_DEBUG_WRITE(0, 'begin '//subname)

   block_first = 1
   block_last = npes

end subroutine get_block_bounds_d


!-----------------------------------------------------------------------
!  routine get_block_gcol_cnt_d
!
!> \brief Return number of dynamics columns in a block
!> \details
!>  Returns the number of dynamics columns in the block with the specified
!>  global block ID.
!>
!>  Will the blockid values be only among those block IDs owned by the calling
!>  MPI task, or can the blockid be the ID of a block owned by any MPI task?
!
!-----------------------------------------------------------------------
integer function get_block_gcol_cnt_d( blockid )

   integer, intent(in) :: blockid

   character(len=*), parameter :: subname = 'dyn_grid::get_block_gcol_cnt_d'


!   MPAS_DEBUG_WRITE(0, 'begin '//subname)

   get_block_gcol_cnt_d = num_col_per_block(blockid)

end function get_block_gcol_cnt_d


!-----------------------------------------------------------------------
!  routine get_block_gcol_d
!
!> \brief Return list of dynamics column indices in a block
!> \details
!>  Return list of global dynamics column indices in the block with
!>  the specified global block ID.
!>
!>  Will the blockid values be among only those block IDs owned by the calling
!>  MPI task, or can the blockid be the ID of a block owned by any MPI task?
!
!-----------------------------------------------------------------------
subroutine get_block_gcol_d(blockid, asize, cdex)

   integer, intent(in) :: blockid      ! global block id
   integer, intent(in) :: asize        ! array size

   integer, intent(out):: cdex(asize)  ! global column indices

   integer :: icol

   character(len=*), parameter :: subname = 'dyn_grid::get_block_gcol_d'


!   MPAS_DEBUG_WRITE(0, 'begin '//subname)

   do icol = 1, num_col_per_block(blockid)
      cdex(icol) = col_indices_in_block(icol, blockid)
   end do
   do icol = num_col_per_block(blockid)+1, asize
      cdex(icol) = 0
   end do

end subroutine get_block_gcol_d
   
   
!-----------------------------------------------------------------------
!  routine get_block_lvl_cnt_d
!
!> \brief Return number of levels in a column
!> \details
!>  Returns the number of levels in the specified column of the specified block.
!>  If column includes surface fields, then it is defined to also
!>  include level 0.
!>
!>  Will the blockid values be among only those block IDs owned by the calling
!>  MPI task, or can the blockid be the ID of a block owned by any MPI task?
!>
!>  Is bcid a global column ID, or a column ID in the local index space of
!>  the block?
!
!-----------------------------------------------------------------------
integer function get_block_lvl_cnt_d(blockid, bcid)

   integer, intent(in) :: blockid  ! global block id
   integer, intent(in) :: bcid     ! column index within block

   character(len=*), parameter :: subname = 'dyn_grid::get_block_lvl_cnt_d'


!   MPAS_DEBUG_WRITE(0, 'begin '//subname)

   ! At present, all blocks have the same number of levels
   get_block_lvl_cnt_d = plevp

end function get_block_lvl_cnt_d


!-----------------------------------------------------------------------
!  routine get_block_levels_d
!
!> \brief Return level indices in a column
!> \details
!>  Returns the level indices in the column of the specified global block.
!>
!>  Will the blockid values be among only those block IDs owned by the calling
!>  MPI task, or can the blockid be the ID of a block owned by any MPI task?
!>
!>  Is bcid a global column ID, or a column ID in the local index space of
!>  the block?
!
!-----------------------------------------------------------------------
subroutine get_block_levels_d(blockid, bcid, lvlsiz, levels)

   integer, intent(in) :: blockid  ! global block id
   integer, intent(in) :: bcid    ! column index within block
   integer, intent(in) :: lvlsiz   ! dimension of levels array

   integer, intent(out) :: levels(lvlsiz) ! levels indices for block

   integer :: k
   character(len=128) :: errmsg

   character(len=*), parameter :: subname = 'dyn_grid::get_block_levels_d'


!   MPAS_DEBUG_WRITE(0, 'begin '//subname)

   if ( lvlsiz < plev + 1 ) then
      write(errmsg,*) ': levels array not large enough (', lvlsiz,' < ',plev + 1,')'
      call endrun( subname // trim(errmsg) )
   else
      do k = 0, plev
         levels(k+1) = k
      end do
      do k = plev+2, lvlsiz
         levels(k) = -1
      end do
   end if

end subroutine get_block_levels_d

!=========================================================================================

integer function get_gcol_block_cnt_d(gcol)

   ! Return number of blocks containing data for the vertical column
   ! with the specified global column index.

   integer, intent(in) :: gcol     ! global column index

   character(len=*), parameter :: subname = 'dyn_grid::get_gcol_block_cnt_d'
   !----------------------------------------------------------------------------

   ! For MPAS, the physics grid is the cell center grid.  Each column is contained in
   ! one cell, and hence in one block.
   get_gcol_block_cnt_d = 1

end function get_gcol_block_cnt_d

!=========================================================================================

subroutine get_gcol_block_d(gcol, cnt, blockid, bcid, localblockid)

   ! Return global block index and local column index for a global column index.
   !
   ! Can this routine be called for global columns that are not owned by
   ! the calling task?

   integer, intent(in) :: gcol     ! global column index
   integer, intent(in) :: cnt      ! size of blockid and bcid arrays

   integer, intent(out) :: blockid(cnt) ! block index
   integer, intent(out) :: bcid(cnt)    ! column index within block
   integer, intent(out), optional :: localblockid(cnt)

   integer :: j

   character(len=*), parameter :: subname = 'dyn_grid::get_gcol_block_d'
   !----------------------------------------------------------------------------

   if ( cnt < 1 ) then
      call endrun( subname // ':: arrays not large enough' )
   end if

   blockid(1) = global_blockid(gcol)
   bcid(1) = local_col_index(gcol)

   do j=2,cnt
      blockid(j) = -1
      bcid(j)    = -1
   end do

end subroutine get_gcol_block_d


!-----------------------------------------------------------------------
!  routine get_block_owner_d
!
!> \brief Return ID of task that owns a block
!> \details
!>  Returns the ID of the task that owns the indicated global block.
!>
!>  Should we assume that task IDs are 0-based (as in MPI)?
!
!-----------------------------------------------------------------------
integer function get_block_owner_d(blockid)

   integer, intent(in) :: blockid  ! global block id

   character(len=*), parameter :: subname = 'dyn_grid::get_block_owner_d'


!   MPAS_DEBUG_WRITE(0, 'begin '//subname)

   get_block_owner_d = (blockid - 1)

end function get_block_owner_d


!-----------------------------------------------------------------------
!  routine get_horiz_grid_dim_d
!
!> \brief Return declared horizontal dimensions of computational grid
!> \details
!>  Return declared horizontal dimensions of computational grid.
!>  For non-lon/lat grids, declare grid to be one-dimensional,
!>  i.e., (ncols x 1).
!>
!>  Is this the global dimension, or the task-local dimension?
!>  I.e., is the number of cells the total number of cells in the mesh
!>  or just the number of cells in blocks owned by this MPI task?
!
!-----------------------------------------------------------------------
subroutine get_horiz_grid_dim_d(hdim1_d, hdim2_d)

   integer, intent(out) :: hdim1_d             ! first horizontal dimension
   integer, intent(out), optional :: hdim2_d   ! second horizontal dimension

   character(len=*), parameter :: subname = 'dyn_grid::get_horiz_grid_dim_d'


!   MPAS_DEBUG_WRITE(0, 'begin '//subname)

   hdim1_d = nCells_g

   if( present(hdim2_d) ) hdim2_d = 1

end subroutine get_horiz_grid_dim_d


!-----------------------------------------------------------------------
!  routine get_horiz_grid_d
!
!> \brief Returns lat, lon, area, and interp weight for a global column
!> \details
!>  Return latitude and longitude (in radians), column surface
!>  area (in radians squared) and surface integration weights
!>  for global column indices that will be passed to/from physics.
!>
!>  Can this routine be passed global column IDs for columns that are not
!>  owned by the calling MPI task?
!>
!>  What do the interpolation weights represent, or how are they used?
!
!-----------------------------------------------------------------------
subroutine get_horiz_grid_d(nxy, clat_d_out, clon_d_out, area_d_out, &
       wght_d_out, lat_d_out, lon_d_out)

   integer, intent(in) :: nxy                     ! array sizes

   real(r8), intent(out), optional :: clat_d_out(:) ! column latitudes
   real(r8), intent(out), optional :: clon_d_out(:) ! column longitudes
   real(r8), intent(out), target, optional :: area_d_out(:)
   real(r8), intent(out), target, optional :: wght_d_out(:) !  weight
   real(r8), intent(out), optional :: lat_d_out(:)  ! column degree latitudes
   real(r8), intent(out), optional :: lon_d_out(:)  ! column degree longitudes

   character(len=*), parameter :: subname = 'dyn_grid::get_horiz_grid_d'


!   MPAS_DEBUG_WRITE(0, 'begin '//subname)

   if ( nxy /= nCells_g ) then
      call endrun( subname // ':: incorrect number of cells' )
   end if

   if ( present( clat_d_out ) ) then
      clat_d_out(:) = latCell_g(:)
   end if

   if ( present( clon_d_out ) ) then
      clon_d_out(:) = lonCell_g(:)
   end if

   if ( present( area_d_out ) ) then
      area_d_out(:) = areaCell_g(:) / (6371229.0_r8**2.0_r8)
   end if

   if ( present( wght_d_out ) ) then
      wght_d_out(:) = areaCell_g(:) / (6371229.0_r8**2.0_r8)
   end if

   if ( present( lat_d_out ) ) then
      lat_d_out(:) = latCell_g(:) * rad2deg
   end if

   if ( present( lon_d_out ) ) then
      lon_d_out(:) = lonCell_g(:) * rad2deg
   end if

end subroutine get_horiz_grid_d


!-----------------------------------------------------------------------
!  routine physgrid_copy_attributes_d
!
!> \brief Create list of attributes for physics grid to copy from dynamics grid
!> \details
!>  Create list of attributes for the physics grid that should be copied
!>  from the corresponding grid object on the dynamics decomposition
!>
!>  The purpose of this routine is not entirely clear.
!
!-----------------------------------------------------------------------
subroutine physgrid_copy_attributes_d(gridname, grid_attribute_names)

   use cam_grid_support, only: max_hcoordname_len

   character(len=max_hcoordname_len),          intent(out) :: gridname
   character(len=max_hcoordname_len), pointer, intent(out) :: grid_attribute_names(:)

   character(len=*), parameter :: subname = 'dyn_grid::physgrid_copy_attributes_d'


!   MPAS_DEBUG_WRITE(0, 'begin '//subname)

   gridname = 'mpas_cell'
   allocate(grid_attribute_names(1))
   grid_attribute_names(1) = 'area'

end subroutine physgrid_copy_attributes_d


!-----------------------------------------------------------------------
!  routine get_dyn_grid_parm_real1d
!
!> \brief Not used for unstructured grids
!> \details
!>  This routine is not used for unstructured grids, but still needed as a
!>  dummy interface to satisfy references from mo_synoz.F90 and phys_gmean.F90
!>
!>  If this routine is unused but called, do we need to ensure, e.g., that
!>  rval is nullified before returning?
!
!-----------------------------------------------------------------------
function get_dyn_grid_parm_real1d(name) result(rval)

   character(len=*), intent(in) :: name
   real(r8), pointer :: rval(:)

   character(len=*), parameter :: subname = 'dyn_grid::get_dyn_grid_parm_real1d'


!   MPAS_DEBUG_WRITE(0, 'begin '//subname)

!   if(name.eq.'w') then
!      call endrun('get_dyn_grid_parm_real1d: w not defined')
!   else if(name.eq.'clat') then
!      call endrun('get_dyn_grid_parm_real1d: clat not supported, use get_horiz_grid_d')
!   else if(name.eq.'latdeg') then
!      call endrun('get_dyn_grid_parm_real1d: latdeg not defined')
!   else
!      nullify(rval)
!   end if

end function get_dyn_grid_parm_real1d


!-----------------------------------------------------------------------
!  routine get_dyn_grid_parm
!
!> \brief Deprecated
!> \details
!>  This function is in the process of being deprecated, but is still needed
!>  as a dummy interface to satisfy external references from some chemistry routines.
!> 
!>  Until this routine is deleted, what values must be returned?
!
!-----------------------------------------------------------------------
integer function get_dyn_grid_parm(name) result(ival)

   character(len=*), intent(in) :: name

   character(len=*), parameter :: subname = 'dyn_grid::get_dyn_grid_parm'


!   MPAS_DEBUG_WRITE(0, 'begin '//subname)

!   if (name == 'plat') then
!      ival = 1
!   else if (name == 'plon') then
!      ival = nCells_g
!   else if(name == 'plev') then
!      ival = plev
!   else	
!      ival = -1
!   end if

end function get_dyn_grid_parm


!-----------------------------------------------------------------------
!  routine dyn_grid_get_colndx
!
!> \brief Not sure?
!> \details
!>  The purpose of this routine is unclear. Does it essentially just call
!>  get_gcol_block_d and get_block_owner_d for an array of global columns?
!>
!>  As with get_gcol_block_d, can this routine be called for global columns
!>  that are not owned by the calling task?
!>
!>  When is this routine needed? Previous implementations suggested that it may
!>  never be called for MPAS.
!
!-----------------------------------------------------------------------
subroutine dyn_grid_get_colndx(igcol, ncols, owners, col, lbk )

   integer, intent(in)  :: ncols
   integer, intent(in)  :: igcol(ncols)
   integer, intent(out) :: owners(ncols)
   integer, intent(out) :: col(ncols)
   integer, intent(out) :: lbk(ncols)

   integer  :: i
   integer :: blockid(1), bcid(1), lclblockid(1)

   character(len=*), parameter :: subname = 'dyn_grid::dyn_grid_get_colndx'


!   MPAS_DEBUG_WRITE(0, 'begin '//subname)

!     do i = 1,ncols
!  
!       call  get_gcol_block_d( igcol(i), 1, blockid, bcid, lclblockid )
!       owners(i) = get_block_owner_d(blockid(1))
!  
!       if ( iam==owners(i) ) then
!          lbk(i) = lclblockid(1)
!          col(i) = bcid(1)
!       else
!          lbk(i) = -1
!          col(i) = -1
!       end if
!  
!    end do

   call endrun('dyn_grid_get_colndx not supported for mpas dycore')

end subroutine dyn_grid_get_colndx

!=========================================================================================

subroutine dyn_grid_get_elem_coords(ie, rlon, rlat, cdex )

   ! Returns the latitude and longitude coordinates, as well as global IDs,
   ! for the columns in a block.

   integer, intent(in) :: ie ! block index

   real(r8),optional, intent(out) :: rlon(:) ! longitudes of the columns in the block
   real(r8),optional, intent(out) :: rlat(:) ! latitudes of the columns in the block
   integer, optional, intent(out) :: cdex(:) ! global column index

   character(len=*), parameter :: subname = 'dyn_grid::dyn_grid_get_elem_coords'
   !----------------------------------------------------------------------------

   ! This routine is called for history output when local time averaging is requested
   ! for a field on a dynamics decomposition.  The code in hbuf_accum_addlcltime appears
   ! to also assume that the field is on the physics grid since there is no argument
   ! passed to specify which dynamics grid the coordinates are for.
   
   call endrun('dyn_grid_get_elem_coords: not implemented for the MPAS grids')

end subroutine dyn_grid_get_elem_coords

!=========================================================================================
! Private routines.
!=========================================================================================

subroutine setup_time_invariant(fh_ini)

   ! Initialize all time-invariant fields needed by the MPAS-Atmosphere dycore,
   ! either by reading these fields from CAM's initial file or by computing them
   ! At present, all time-invariant fields are read from the file descriptor fh_ini,
   ! but in future, some of these fields could be computed
   ! here based on other fields that were read

   ! Arguments
   type(file_desc_t), pointer :: fh_ini

   ! Local variables
   type(mpas_pool_type),   pointer :: meshPool
   real(r8), pointer     :: rdzw(:)
   real(r8), allocatable :: dzw(:)

   integer :: k, kk

   character(len=*), parameter :: routine = 'dyn_grid::setup_time_invariant'
   !----------------------------------------------------------------------------

   ! Read time-invariant fields
   call cam_mpas_read_static(fh_ini, endrun)

   ! Compute unit vectors giving the local north and east directions as well as
   ! the unit normal vector for edges
   call cam_mpas_compute_unit_vectors()

   ! Access dimensions that are made public via this module
   call mpas_pool_get_subpool(domain_ptr % blocklist % structs, 'mesh', meshPool)
   call mpas_pool_get_dimension(meshPool, 'nCellsSolve', nCellsSolve)
   call mpas_pool_get_dimension(meshPool, 'nEdgesSolve', nEdgesSolve)
   call mpas_pool_get_dimension(meshPool, 'nVerticesSolve', nVerticesSolve)
   call mpas_pool_get_dimension(meshPool, 'nVertLevels', nVertLevelsSolve) ! MPAS always solves over the full column

   ! check that number of vertical layers matches MPAS grid data
   if (plev /= nVertLevelsSolve) then
      write(iulog,*) routine//': ERROR: number levels in IC file does not match plev: file, plev=', &
                     nVertLevelsSolve, plev
      call endrun(routine//': ERROR: number levels in IC file does not match plev.')
   end if

   ! Compute the zeta coordinate at layer interfaces and midpoints.  Store
   ! in arrays using CAM vertical index order for use in CAM coordinate objects.
   call mpas_pool_get_array(meshPool, 'rdzw', rdzw)

   allocate(dzw(plev))
   dzw = 1._r8 / rdzw
   zw(plev+1) = 0._r8
   do k = plev, 1, -1
      kk = plev - k + 1
      zw(k) = zw(k+1) + dzw(kk)
      zw_mid(k) = 0.5_r8 * (zw(k+1) + zw(k))
   end do

   deallocate(dzw)

end subroutine setup_time_invariant

!=========================================================================================

subroutine define_cam_grids()

   ! Defines the dynamics and physics grids on the dynamics decompostion.
   ! The physics grid on the physics decomposition is defined in phys_grid_init.

   use cam_grid_support, only: horiz_coord_t, horiz_coord_create, iMap
   use cam_grid_support, only: cam_grid_register, cam_grid_attribute_register
 
   ! Local variables
   integer :: i, j

   type(horiz_coord_t), pointer     :: lat_coord
   type(horiz_coord_t), pointer     :: lon_coord
   integer(iMap),       allocatable :: gidx(:)        ! global indices
   integer(iMap),       pointer     :: grid_map(:,:)

   type(mpas_pool_type),   pointer :: meshPool

   integer,  dimension(:), pointer :: indexToCellID ! global indices of cell centers
   real(r8), dimension(:), pointer :: latCell   ! cell center latitude (radians)
   real(r8), dimension(:), pointer :: lonCell   ! cell center longitude (radians)
   real(r8), dimension(:), pointer :: areaCell  ! cell areas in m^2
   real(r8), dimension(:), pointer :: area_unit ! cell areas on unit sphere (radians^2)

   integer,  dimension(:), pointer :: indexToEdgeID ! global indices of edge nodes
   real(r8), dimension(:), pointer :: latEdge   ! edge node latitude (radians)
   real(r8), dimension(:), pointer :: lonEdge   ! edge node longitude (radians)

   character(len=*), parameter :: subname = 'dyn_grid::define_cam_grids'
   !----------------------------------------------------------------------------

   MPAS_DEBUG_WRITE(0, 'begin '//subname)
 
   call mpas_pool_get_subpool(domain_ptr % blocklist % structs, 'mesh', meshPool)

   !-------------------------------------------------------------!
   ! Construct coordinate and grid objects for cell center grid. !
   !-------------------------------------------------------------!

   call mpas_pool_get_array(meshPool, 'indexToCellID', indexToCellID)
   call mpas_pool_get_array(meshPool, 'latCell', latCell)
   call mpas_pool_get_array(meshPool, 'lonCell', lonCell)
   call mpas_pool_get_array(meshPool, 'areaCell', areaCell)

   allocate(gidx(nCellsSolve))
   gidx = indexToCellID(1:nCellsSolve)

   lat_coord => horiz_coord_create('lat', 'ncol', nCells_g, 'latitude',      &
          'degrees_north', 1, nCellsSolve, latCell(1:nCellsSolve)*rad2deg, map=gidx)
   lon_coord => horiz_coord_create('lon', 'ncol', nCells_g, 'longitude',     &
          'degrees_east', 1, nCellsSolve, lonCell(1:nCellsSolve)*rad2deg, map=gidx)
 
   ! Map for cell centers grid
   allocate(grid_map(3, nCellsSolve))
   do i = 1, nCellsSolve
      grid_map(1, i) = i
      grid_map(2, i) = 1
      grid_map(3, i) = gidx(i)
   end do

   ! cell center grid
   call cam_grid_register('mpas_cell', dyn_decomp, lat_coord, lon_coord,     &
          grid_map, block_indexed=.false., unstruct=.true.)
   allocate(area_unit(nCellsSolve))
   area_unit = areaCell(1:nCellsSolve) / 6371229.0_r8**2.0_r8
   call cam_grid_attribute_register('mpas_cell', 'area', 'cell areas (radian^2)',  &
          'ncol', area_unit, gidx)

   ! gidx can be deallocated.  Values are copied into the coordinate and attribute objects.
   deallocate(gidx)

   ! grid_map memory cannot be deallocated.  The cam_filemap_t object just points
   ! to it.  Pointer can be disassociated.
   nullify(grid_map) ! Map belongs to grid now

   ! area_unit memory cannot be deallocated.  The cam_grid_attribute_1d_r8_t object points
   ! to it.  Pointer can be disassociated.
   nullify(area_unit)

   ! pointers to coordinate object can be nullified.  Memory is now pointed to by the
   ! grid object.
   nullify(lat_coord)
   nullify(lon_coord)

   !-----------------------------------------------------------!
   ! Construct coordinate and grid objects for edge node grid. !
   !-----------------------------------------------------------!

   call mpas_pool_get_array(meshPool, 'indexToEdgeID', indexToEdgeID)
   call mpas_pool_get_array(meshPool, 'latEdge', latEdge)
   call mpas_pool_get_array(meshPool, 'lonEdge', lonEdge)

   allocate(gidx(nEdgesSolve))
   gidx = indexToEdgeID(1:nEdgesSolve)

   lat_coord => horiz_coord_create('lat_edge', 'nedge', nEdges_g, 'latitude',      &
          'degrees_north', 1, nEdgesSolve, latEdge(1:nEdgesSolve)*rad2deg, map=gidx)
   lon_coord => horiz_coord_create('lon_edge', 'nedge', nEdges_g, 'longitude',     &
          'degrees_east', 1, nEdgesSolve, lonEdge(1:nEdgesSolve)*rad2deg, map=gidx)
 
   ! Map for edge node grid
   allocate(grid_map(3, nEdgesSolve))
   do i = 1, nEdgesSolve
      grid_map(1, i) = i
      grid_map(2, i) = 1
      grid_map(3, i) = gidx(i)
   end do

   ! Edge node grid object
   call cam_grid_register('mpas_edge', edge_decomp, lat_coord, lon_coord,     &
          grid_map, block_indexed=.false., unstruct=.true.)

   deallocate(gidx)
   nullify(grid_map)
   nullify(lat_coord)
   nullify(lon_coord)
 
end subroutine define_cam_grids

end module dyn_grid
